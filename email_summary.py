# -*- coding: utf-8 -*-
"""email_summary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubwrjk9k3rCJkTDUgeXG70ARH5KGWOTO
"""

import streamlit as st
st.write('# Email Summerizer')

import os
from haystack.document_stores import ElasticsearchDocumentStore

# Get the host where Elasticsearch is running, default to localhost
host = os.environ.get("ELASTICSEARCH_HOST", "localhost")

document_store = ElasticsearchDocumentStore(host=host, username="", password="", index="document")

# Define the directory to store the document
doc_dir = "data/your_documents"
from shutil import copyfile
os.makedirs(doc_dir, exist_ok=True)

# Path to your own text file
your_text_file = "/content/email_thread_details.json"

# Copy your text file to the document directory
destination_file = os.path.join(doc_dir, "email_thread_details.json")
copyfile(your_text_file, destination_file)

# Check if the file was copied successfully
if os.path.exists(destination_file):
    print(f"File {your_text_file} copied to {destination_file}")
else:
    print("File copy failed. Please check the file paths.")

from haystack import Pipeline
from haystack.nodes import TextConverter, PreProcessor

indexing_pipeline = Pipeline()
text_converter = TextConverter()
preprocessor = PreProcessor(
    clean_whitespace=True,
    clean_header_footer=True,
    clean_empty_lines=True,
    split_by="word",
    split_length=200,
    split_overlap=20,
    split_respect_sentence_boundary=True,
)

indexing_pipeline.add_node(component=text_converter, name="TextConverter", inputs=["File"])
indexing_pipeline.add_node(component=preprocessor, name="PreProcessor", inputs=["TextConverter"])
indexing_pipeline.add_node(component=document_store, name="DocumentStore", inputs=["PreProcessor"])

files_to_index = [doc_dir + "/" + f for f in os.listdir(doc_dir)]
indexing_pipeline.run_batch(file_paths=files_to_index)

from haystack.nodes import BM25Retriever

retriever = BM25Retriever(document_store=document_store)

from haystack.nodes import FARMReader

reader = FARMReader(model_name_or_path="deepset/roberta-base-squad2", use_gpu=True)

from haystack import Pipeline

querying_pipeline = Pipeline()
querying_pipeline.add_node(component=retriever, name="Retriever", inputs=["Query"])
querying_pipeline.add_node(component=reader, name="Reader", inputs=["Retriever"])

# Initialize flag to control the loop
flag = True

# Main loop
count = 0
while flag:
    # Get user input
    user_input =st.text_input("You: ",key=count).lower()
    count+=1

    # Check for exit conditions
    
    if user_input in ['bye', 'thank you']:
          flag = False
          st.write("Chatbot: You're welcome. Goodbye!")
    elif user_input == 'hi':
          st.write("Chatbot: Hi!")
    else:
          # Run the prediction with the user-entered query
          prediction = querying_pipeline.run(
              query=user_input,
              params={"Retriever": {"top_k": 1}, "Reader": {"top_k": 1}}
          )

          # Extract answer  from the prediction        
          answer = prediction['answers'][0].answer

          st.write("Result:", answer)